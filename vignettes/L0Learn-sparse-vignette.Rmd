---
title: "L0Learn Sparse Support Vignette"
author:
- name: Hussein Hazimeh and Rahul Mazumder and Tim Nonet
- name: Massachusetts Institute of Technology (MIT)
date: "`r Sys.Date()`"
output:
  html_document:
    theme: united
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
vignette: >
  %\VignetteIndexEntry{L0Learn Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(comment = "#>", warning=FALSE, message=FALSE)
```
```{r echo = FALSE}
# Thanks to Yihui Xie for providing this code
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
   lines <- options$output.lines
   if (is.null(lines)) {
     return(hook_output(x, options))  # pass to default hook
   }
   x <- unlist(strsplit(x, "\n"))
   more <- "..."
   if (length(lines)==1) {        # first n lines
     if (length(x) > lines) {
       # truncate the output, but add ....
       x <- c(head(x, lines), more)
     }
   } else {
     x <- c(more, x[lines], more)
   }
   # paste these lines together
   x <- paste(c(x, ""), collapse = "\n")
   hook_output(x, options)
 })
```

# Sparse Functionality
L0Learn has been expanded to support Column Compressed Sparse Matrices (CSC) know in R as 'dgCMatrix'.

## Compare Sparse and Dense Matrix Answers
*Note* While for any given size a sparse matrix solution will be slower than a dense matrix solution, a sparse matrix can scale much larger (assuming the data being stored is sparse).

Using the same dataset as before in [INSERT HTML FILE]

* X is a 500x1000 design matrix with iid standard normal entries
* B is a 1000x1 vector with the first 10 entries set to 1 and the rest are zeros.
* e is a 500x1 vector with iid standard normal entries
* y is a 500x1 response vector such that y  = XB + e

This dataset can be generated in R as follows:
```{r}
set.seed(1) # fix the seed to get a reproducible result
X = matrix(rnorm(500*1000),nrow=500,ncol=1000)
B = c(rep(1,10),rep(0,990))
e = rnorm(500)
y = X%*%B + e
```
We can create a sparse version of X below. While this is terribly inefficient usage of a sparse matrix (as X is fully dense), it shows that sparse support is valid. Section [Link to Header](#Compare Sparse and Dense Matrix Run/Memory Time) shows the benefits gained from a sparse matrix.
```{r}
library(Matrix)
X_sparse <- as(X, "dgCMatrix")   
```

Just as in '[INSERT HTML FILE]]' We will use L0Learn to estimate B from the data (y,X) and (y, X_sparse). We fit a solutions for the L0-regularized model with at most 20 non-zeros using coordinate descent (CD), we use the `L0Learn.fit` function as follows for both the sparse and non sparse X:
```{r}
library(L0Learn)
fit <- L0Learn.fit(X, y, penalty="L0", maxSuppSize=20)
fit_sparse <- L0Learn.fit(X_sparse, y, penalty="L0", maxSuppSize=20)
```
This will generate solutions for a sequence of $\lambda$ values (chosen automatically by the algorithm) for each `L0Learn.fit`. However, due to random number generators we will see that there are slight differences but they mostly agree.
```{r}
print(fit)
print(fit_sparse)
```

## Compare Sparse and Dense Matrix Run/Memory Times

Creating larger design Matrices

N is the number of samples
P is the number of features

* X is a NXP design matrix with iid standard normal entries
* B is a Px1 vector with the first 10 entries set to 1 and the rest are zeros.
* e is a Nx1 vector with iid standard normal entries
* y is a Nx1 response vector such that y  = XB + e

These dataset can be generated in R as follows:
```{r}
dataset <- function(number_of_samples, number_of_features,
                    coef_ratio = .01, sparseness = .95) {
   set.seed(1) # fix the seed to get a reproducible result
    X = matrix(rnorm(number_of_samples*number_of_features),
               nrow=number_of_samples,ncol=number_of_features)
    X[abs(X) < sparseness] <- 0.
    print(paste0("X is ", 100*sum(X == 0)/(number_of_samples*number_of_features), " % sparse"))
    number_of_ones = as.integer(number_of_features*coef_ratio)
    number_of_zeros = number_of_features - number_of_ones
    B = c(rep(1,number_of_ones),rep(0,number_of_zeros))
    e = rnorm(number_of_samples)
    y = X%*%B + e
    return(list(X, B, e, y))
}
```


```{r}
tmp <-  dataset(2000, 20000, sparseness = 2.1)
X <- tmp[[1]]
B <- tmp[[2]]
e <- tmp[[3]]
y <- tmp[[4]]

X_sparse <- as(X, "dgCMatrix")  
```

```{r, results="hide"}
library(devtools)
devtools::install_github("eddelbuettel/rbenchmark")
library(rbenchmark)
```

```{r}
benchmark("dense" =  { fit <- L0Learn.fit(X, y, penalty="L0", maxSuppSize=20)},
          "sparse" = {fit_sparse <- L0Learn.fit(X_sparse, y, penalty="L0", maxSuppSize=20)},
          replications = 10,
          columns = c("test", "replications", "elapsed",
                      "relative", "user.self", "sys.self"))

```


```{r}
tmp <-  dataset(4000, 40000, sparseness = 2.1)
X <- tmp[[1]]
B <- tmp[[2]]
e <- tmp[[3]]
y <- tmp[[4]]

X_sparse <- as(X, "dgCMatrix")  
```

```{r}
benchmark("dense" = { fit <- L0Learn.fit(X, y, penalty="L0", maxSuppSize=20)},
          "sparse" = {fit_sparse <- L0Learn.fit(X_sparse, y, penalty="L0", maxSuppSize=20)},
          replications = 10,
          columns = c("test", "replications", "elapsed",
                      "relative", "user.self", "sys.self"))

```

Memory Profiling:
http://minimallysufficient.github.io/r/programming/c++/2018/02/16/profiling-rcpp-packages.html
    Does not work as it does not profile Rcpp Code.
    
Current plan is to use:
https://stackoverflow.com/questions/30172820/lineprof-equivalent-for-rcpp

```{r, results="hide"}
install.packages("profmem", repos = "http://cran.us.r-project.org")
library(profmem)
```

```{r}
p <- profmem({ fit <- L0Learn.fit(X, y, penalty="L0", maxSuppSize=20)})
p
```